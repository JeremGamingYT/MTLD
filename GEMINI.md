# GEMINI.md: MTLD Project

## Project Overview

This project, titled **MTLD (Modèle à Trajectoire Latente Déterministe pour la Restitution Séquentielle d'Anime)**, is a deep learning system for sequential anime frame generation. It is built using Python and the PyTorch framework.

The primary goal of MTLD is to predict and generate a sequence of future video frames based on a single initial input frame (a "priming" image).

The core architecture is a **Generative Adversarial Network (GAN)** with a unique temporal component:

*   **Encoder:** A convolutional neural network (CNN) that encodes an input image into a low-dimensional latent vector (`z`).
*   **Decoder:** A transposed convolutional network that reconstructs an image from a latent vector.
*   **Conditional Latent Trajectory Generator:** This is the key innovation. It uses a Gated Recurrent Unit (GRU) network to model the "trajectory" of the sequence in the latent space. Given a starting latent vector `z_start` (from the encoder), it predicts a series of subsequent latent vectors (`z_2, z_3, ...`).
*   **PatchDiscriminator:** A CNN-based discriminator that distinguishes between real image sequences and the sequences generated by the model. This pushes the generator to create more realistic and coherent frames.

The project has evolved through several versions, with `kaggle.py` (v1.8) being the most recent and feature-rich script, including optimizations like pre-loading the dataset into RAM and GPU monitoring to prevent overheating during long training sessions.

## Dependencies

The project relies on several Python libraries. You can install them using pip.

```bash
# It is recommended to use a virtual environment
python -m venv .venv
source .venv/bin/activate # On Linux/macOS
# .\\.venv\\Scripts\\activate # On Windows

pip install torch torchvision
pip install lpips
pip install opencv-python
pip install pillow
pip install tqdm
pip install numpy
pip install nvidia-ml-py # For GPU monitoring
```

## Dataset Setup

The model is trained on sequences of image frames extracted from videos. The expected directory structure is as follows:

```
<dataset_root>/
├── arc_1/
│   ├── frame_0000.png
│   ├── frame_0001.png
│   └── ...
├── arc_2/
│   ├── frame_0000.png
│   ├── frame_0001.png
│   └── ...
└── ...
```

1.  Create a root directory for your dataset.
2.  Inside, create a separate sub-directory for each video sequence (referred to as an "arc" in the code).
3.  Each sub-directory must contain the video frames as sequentially numbered PNG images. The code uses a regular expression `(\d+)` to extract the number from the filename for sorting.
4.  Update the `DATASET_PATH` variable in the `Config` class within the training script (e.g., `kaggle.py`) to point to your `<dataset_root>` directory.

The repository contains helper scripts like `download_videos.py` and `extract_frames.py` that may assist in this process.

## Building and Running

The project does not require a formal build process. The scripts can be executed directly using a Python interpreter. `kaggle.py` is the recommended script for training and generation.

### Training

1.  **Configure:** Open `kaggle.py` and review the `Config` class. At a minimum, you must set `DATASET_PATH` to the location of your prepared dataset. You may also adjust hyperparameters like `BATCH_SIZE`, `EPOCHS`, and `LEARNING_RATE`.
2.  **Run:** Execute the script from your terminal. The `if __name__ == '__main__':` block is set up to call the `train_mtld()` function by default.

    ```bash
    python kaggle.py
    ```

3.  **Checkpoints & Outputs:**
    *   Model checkpoints (`.pth` files) will be saved to the directory specified by `MODEL_SAVE_PATH` (e.g., `./models_mtld_v1.8/`).
    *   Comparison images will be saved to the `OUTPUT_SAVE_PATH` (e.g., `./outputs_mtld_v1.8/`).

### Generation (Inference)

To generate a new sequence from a trained model:

1.  **Configure:** In `kaggle.py`, comment out the call to `train_mtld()` in the `if __name__ == '__main__':` block and uncomment the "MODE 2: GÉNÉRATION CONDITIONNELLE" section.
2.  **Set Paths:**
    *   `model_file`: Set this to the path of your trained checkpoint (`.pth` file).
    *   `priming_image_path`: Set this to the path of the initial image you want to start the generation from.
    *   `num_frames_to_generate`: Specify how many frames you want to create.
3.  **Run:** Execute the script.

    ```bash
    python kaggle.py
    ```

4.  **Outputs:** The generated frames, a `.mp4` video, and a `.gif` will be saved in the directory specified by `OUTPUT_SAVE_PATH`.

## Development Conventions

*   **Configuration:** All major hyperparameters and paths are centralized in a `Config` class at the top of each script.
*   **Versioning:** The main scripts (`main.py`, `train_distributed.py`, `kaggle.py`) represent different stages of the project's development and are versioned in their docstrings and filenames.
*   **Modularity:** The architecture is broken down into distinct PyTorch `nn.Module` classes (Encoder, Decoder, etc.), promoting code reuse and clarity.
*   **Training Logic:** The main training loop is encapsulated in a `train_mtld()` function.
*   **Inference Logic:** Generation is handled by the `generate_sequence()` function.
*   **Multi-GPU:** `train_distributed.py` provides an implementation for multi-GPU training using `DistributedDataParallel`.
